version: "3.8"

x-proxy-env: &proxy-env
  # Optional proxy passthrough for model downloads inside containers.
  # On macOS/Windows Docker Desktop, host loopback is usually `host.docker.internal`, not 127.0.0.1.
  # Optional HuggingFace endpoint mirror for weights download (e.g. https://hf-mirror.com).
  # If you want to use the official hub, set HF_ENDPOINT=https://huggingface.co in .env.
  HF_ENDPOINT: ${HF_ENDPOINT:-https://hf-mirror.com}
  HTTP_PROXY: ${HTTP_PROXY:-}
  HTTPS_PROXY: ${HTTPS_PROXY:-}
  ALL_PROXY: ${ALL_PROXY:-}
  NO_PROXY: ${NO_PROXY:-localhost,127.0.0.1,::1}
  http_proxy: ${http_proxy:-}
  https_proxy: ${https_proxy:-}
  all_proxy: ${all_proxy:-}
  no_proxy: ${no_proxy:-localhost,127.0.0.1,::1}

services:
  # External diarizer (pyannote) - optional but recommended for meeting speaker turns.
  tingwu-diarizer:
    build:
      context: .
      dockerfile: Dockerfile.diarizer
    image: tingwu-diarizer:pyannote
    profiles: ["diarizer"]
    container_name: tingwu-diarizer
    ports:
      - "${PORT_DIARIZER:-8300}:8000"
    volumes:
      - huggingface-cache:/root/.cache/huggingface
    environment:
      <<: *proxy-env
      HF_TOKEN: ${HF_TOKEN:-}
      DEVICE: ${DIARIZER_DEVICE:-cuda}
      DIARIZER_MODEL: ${DIARIZER_MODEL:-pyannote/speaker-diarization-3.1}
      DIARIZER_WARMUP_ON_STARTUP: ${DIARIZER_WARMUP_ON_STARTUP:-true}
      # Optional: constrain expected speaker count for more stable diarization.
      # Priority: NUM_SPEAKERS > MIN/MAX.
      DIARIZER_NUM_SPEAKERS: ${DIARIZER_NUM_SPEAKERS:-}
      DIARIZER_MIN_SPEAKERS: ${DIARIZER_MIN_SPEAKERS:-}
      DIARIZER_MAX_SPEAKERS: ${DIARIZER_MAX_SPEAKERS:-}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Qwen3-ASR (OpenAI-compatible /v1/chat/completions)
  qwen3-asr:
    image: ${QWEN3_ASR_IMAGE:-qwenllm/qwen3-asr:latest}
    container_name: qwen3-asr
    ports:
      - "9001:8000"
    ipc: host
    volumes:
      - huggingface-cache:/root/.cache/huggingface
      - modelscope-cache:/root/.cache/modelscope
      - ./docker/remote-asr/start_qwen3_asr.sh:/start_qwen3_asr.sh:ro
    # NOTE: the script is mounted read-only; don't try in-place edits (sed -i).
    command: ["bash", "-lc", "tr -d '\\r' < /start_qwen3_asr.sh > /tmp/start_qwen3_asr.sh && chmod +x /tmp/start_qwen3_asr.sh && bash /tmp/start_qwen3_asr.sh"]
    shm_size: "4gb"
    environment:
      <<: *proxy-env
      # Optional: PyPI mirror for installing ModelScope inside the container.
      PIP_INDEX_URL: ${PIP_INDEX_URL:-https://mirrors.aliyun.com/pypi/simple/}
      PIP_TRUSTED_HOST: ${PIP_TRUSTED_HOST:-mirrors.aliyun.com}
      QWEN3_MODEL_ID: ${QWEN3_MODEL_ID:-Qwen/Qwen3-ASR-0.6B}
      # Optional: point to a local model directory inside the container to avoid downloading.
      QWEN3_MODEL_PATH: ${QWEN3_MODEL_PATH:-}
      # vLLM max context length. Keep this smaller by default so the server can
      # start under tighter GPU memory budgets.
      QWEN3_MAX_MODEL_LEN: ${QWEN3_MAX_MODEL_LEN:-32768}
      PORT: 8000
      GPU_MEMORY_UTILIZATION: ${QWEN3_GPU_MEMORY_UTILIZATION:-0.85}
      MODEL_SOURCE: ${MODEL_SOURCE:-modelscope}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # VibeVoice-ASR (OpenAI-compatible /v1/chat/completions)
  # NOTE: the official server defaults (64k context) usually need >=24GB VRAM.
  # This compose file uses smaller defaults so it can start on ~16GB VRAM GPUs for smoke tests.
  vibevoice-asr:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:latest}
    container_name: vibevoice-asr
    ports:
      - "9002:8000"
    ipc: host
    volumes:
      # Mount a local VibeVoice repo directory that contains vllm_plugin + python package.
      # Default: use the vendored snapshot under ./third_party/VibeVoice.
      - ${VIBEVOICE_REPO_PATH:-./third_party/VibeVoice}:/app
      - huggingface-cache:/root/.cache/huggingface
      - modelscope-cache:/root/.cache/modelscope
      - ./docker/remote-asr/start_vibevoice_asr.sh:/start_vibevoice_asr.sh:ro
    entrypoint: ["bash", "-lc"]
    # NOTE: the script is mounted read-only; don't try in-place edits (sed -i).
    command: ["tr -d '\\r' < /start_vibevoice_asr.sh > /tmp/start_vibevoice_asr.sh && chmod +x /tmp/start_vibevoice_asr.sh && bash /tmp/start_vibevoice_asr.sh"]
    shm_size: "4gb"
    environment:
      <<: *proxy-env
      # Optional: use a PyPI mirror inside the container for installing the mounted VibeVoice package.
      PIP_INDEX_URL: ${PIP_INDEX_URL:-https://mirrors.aliyun.com/pypi/simple/}
      PIP_TRUSTED_HOST: ${PIP_TRUSTED_HOST:-mirrors.aliyun.com}
      VIBEVOICE_MODEL_ID: ${VIBEVOICE_MODEL_ID:-microsoft/VibeVoice-ASR}
      # Optional: point to a local model directory inside the container to avoid downloading.
      VIBEVOICE_MODEL_PATH: ${VIBEVOICE_MODEL_PATH:-}
      PORT: 8000
      SERVED_MODEL_NAME: ${VIBEVOICE_SERVED_MODEL_NAME:-vibevoice}
      DTYPE: ${VIBEVOICE_DTYPE:-float16}
      MAX_MODEL_LEN: ${VIBEVOICE_MAX_MODEL_LEN:-16384}
      MAX_NUM_SEQS: ${VIBEVOICE_MAX_NUM_SEQS:-2}
      MAX_NUM_BATCHED_TOKENS: ${VIBEVOICE_MAX_NUM_BATCHED_TOKENS:-4096}
      GPU_MEMORY_UTILIZATION: ${VIBEVOICE_GPU_MEMORY_UTILIZATION:-0.90}
      VIBEVOICE_FFMPEG_MAX_CONCURRENCY: ${VIBEVOICE_FFMPEG_MAX_CONCURRENCY:-16}
      MODEL_SOURCE: ${MODEL_SOURCE:-modelscope}
      PYTORCH_ALLOC_CONF: expandable_segments:True
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # TingWu service configured to route to the two remote ASR servers above.
  tingwu:
    build:
      context: .
      dockerfile: Dockerfile
    image: tingwu-speech-service:pytorch
    container_name: tingwu-service
    ports:
      - "${PORT:-8000}:8000"
    volumes:
      - ./data:/app/data
      - model-cache:/root/.cache/modelscope
      - huggingface-cache:/root/.cache/huggingface
    environment:
      <<: *proxy-env
      DEVICE: ${DEVICE:-cuda}
      NGPU: ${NGPU:-1}
      NCPU: ${NCPU:-4}
      DEBUG: ${DEBUG:-false}
      HOTWORDS_THRESHOLD: ${HOTWORDS_THRESHOLD:-0.85}
      SPEAKER_UNSUPPORTED_BEHAVIOR: ${SPEAKER_UNSUPPORTED_BEHAVIOR:-ignore}
      SPEAKER_EXTERNAL_DIARIZER_ENABLE: ${SPEAKER_EXTERNAL_DIARIZER_ENABLE:-false}
      SPEAKER_EXTERNAL_DIARIZER_BASE_URL: ${SPEAKER_EXTERNAL_DIARIZER_BASE_URL:-http://tingwu-diarizer:8000}
      SPEAKER_EXTERNAL_DIARIZER_TIMEOUT_S: ${SPEAKER_EXTERNAL_DIARIZER_TIMEOUT_S:-60}
      SPEAKER_EXTERNAL_DIARIZER_MAX_TURNS: ${SPEAKER_EXTERNAL_DIARIZER_MAX_TURNS:-200}
      SPEAKER_EXTERNAL_DIARIZER_MAX_TURN_DURATION_S: ${SPEAKER_EXTERNAL_DIARIZER_MAX_TURN_DURATION_S:-25}

      # Remote ASR backend wiring
      ASR_BACKEND: ${ASR_BACKEND:-router}
      QWEN3_ASR_BASE_URL: http://qwen3-asr:8000
      QWEN3_ASR_MODEL: ${QWEN3_MODEL_ID:-Qwen/Qwen3-ASR-0.6B}
      VIBEVOICE_ASR_BASE_URL: http://vibevoice-asr:8000
      VIBEVOICE_ASR_MODEL: ${VIBEVOICE_SERVED_MODEL_NAME:-vibevoice}
      VIBEVOICE_ASR_USE_CHAT_COMPLETIONS_FALLBACK: true
      ROUTER_SHORT_BACKEND: qwen3
      ROUTER_LONG_BACKEND: vibevoice
      ROUTER_LONG_AUDIO_THRESHOLD_S: ${ROUTER_LONG_AUDIO_THRESHOLD_S:-60}
      ROUTER_FORCE_VIBEVOICE_WHEN_WITH_SPEAKER: ${ROUTER_FORCE_VIBEVOICE_WHEN_WITH_SPEAKER:-true}
    depends_on:
      - qwen3-asr
      - vibevoice-asr
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  model-cache:
  huggingface-cache:
  modelscope-cache:
