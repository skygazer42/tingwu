# GGUF 后端 Dockerfile
# 用于 TingWu 的 gguf ASR 后端 (src/models/backends/gguf)。
#
# 说明:
# - GGUF 后端需要额外依赖: onnxruntime + gguf
# - 需要你把模型文件放到 ./data/models (并挂载到容器 /app/data/models)
# - llama.cpp 动态库会在构建镜像时编译并内置到 /app/llama_cpp/lib（默认 GGUF_LIB_DIR 指向该目录）
# - 该镜像以 API 为主，不强制构建前端（可按需加上 frontend-builder 阶段）

ARG TORCH_BASE_IMAGE=pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime
ARG LLAMA_CPP_REPO=https://github.com/ggml-org/llama.cpp.git
ARG LLAMA_CPP_REF=
ARG PIP_INDEX_URL=https://mirrors.aliyun.com/pypi/simple/
ARG PIP_TRUSTED_HOST=mirrors.aliyun.com

FROM ${TORCH_BASE_IMAGE} AS python-builder
ARG PIP_INDEX_URL
ARG PIP_TRUSTED_HOST

WORKDIR /app

# NOTE: Some networks/proxies can corrupt apt's signed InRelease files and cause
# "invalid signature" errors. We try the image default sources first, then fall
# back to a commonly available mirror over HTTP.
RUN (apt-get -o Acquire::Retries=3 update || ( \
          echo "[apt] update failed; falling back to http://mirrors.aliyun.com/ubuntu" >&2; \
          sed -i 's|https://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://mirrors.aliyun.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g' /etc/apt/sources.list; \
          apt-get -o Acquire::Retries=3 update)) \
    && apt-get install -y --no-install-recommends \
      build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .

# 安装基础依赖
RUN pip install --no-cache-dir -i "${PIP_INDEX_URL}" --trusted-host "${PIP_TRUSTED_HOST}" -r requirements.txt

# 安装 GGUF/ONNX 相关依赖
RUN pip install --no-cache-dir \
    -i "${PIP_INDEX_URL}" --trusted-host "${PIP_TRUSTED_HOST}" \
    onnxruntime>=1.16.0 \
    gguf>=0.10.0

FROM ${TORCH_BASE_IMAGE} AS llama-builder

ARG LLAMA_CPP_REPO
ARG LLAMA_CPP_REF

WORKDIR /tmp

RUN (apt-get -o Acquire::Retries=3 update || ( \
          echo "[apt] update failed; falling back to http://mirrors.aliyun.com/ubuntu" >&2; \
          sed -i 's|https://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://mirrors.aliyun.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g' /etc/apt/sources.list; \
          apt-get -o Acquire::Retries=3 update)) \
    && apt-get install -y --no-install-recommends \
      git \
      cmake \
      build-essential \
    && rm -rf /var/lib/apt/lists/*

# Workaround: some networks/proxies break git fetch over HTTP/2 (common with GitHub),
# causing errors like "curl 92 HTTP/2 stream ... CANCEL" / "early EOF".
# Force HTTP/1.1 and retry a few times.
RUN git config --global http.version HTTP/1.1 \
    && for i in 1 2 3; do \
         echo "[git] cloning llama.cpp (attempt ${i}) from ${LLAMA_CPP_REPO}" >&2; \
         git clone --depth 1 "${LLAMA_CPP_REPO}" llama.cpp && break; \
         echo "[git] clone failed; retrying..." >&2; \
         rm -rf llama.cpp; \
         sleep $((i * 2)); \
       done \
    && test -d llama.cpp/.git

WORKDIR /tmp/llama.cpp

# Optional: pin to a specific tag/branch for reproducible builds.
# Example: --build-arg LLAMA_CPP_REF=b3932
RUN if [ -n "${LLAMA_CPP_REF}" ]; then \
      for i in 1 2 3; do \
        echo "[git] fetching llama.cpp ref=${LLAMA_CPP_REF} (attempt ${i})" >&2; \
        git fetch --depth 1 origin "${LLAMA_CPP_REF}" && git checkout FETCH_HEAD && break; \
        echo "[git] fetch failed; retrying..." >&2; \
        sleep $((i * 2)); \
      done; \
    fi

RUN cmake -S . -B build \
      -DCMAKE_BUILD_TYPE=Release \
      -DBUILD_SHARED_LIBS=ON \
    && cmake --build build -j"$(nproc)" \
    && mkdir -p /out \
    && find build -name 'libllama.so' -exec cp -v {} /out/ \; \
    && find build -name 'libggml.so' -exec cp -v {} /out/ \; \
    && find build -name 'libggml-base.so' -exec cp -v {} /out/ \;

FROM ${TORCH_BASE_IMAGE}

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    # 默认使用 gguf 后端
    ASR_BACKEND=gguf

# Default llama.cpp lib dir for the GGUF backend. Note that /app/data is
# usually bind-mounted from the host, so keep libs outside /app/data.
ENV GGUF_LIB_DIR=/app/llama_cpp/lib \
    LD_LIBRARY_PATH=/app/llama_cpp/lib:${LD_LIBRARY_PATH}

RUN (apt-get -o Acquire::Retries=3 update || ( \
          echo "[apt] update failed; falling back to http://mirrors.aliyun.com/ubuntu" >&2; \
          sed -i 's|https://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://archive.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|http://security.ubuntu.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g; s|https://mirrors.aliyun.com/ubuntu|http://mirrors.aliyun.com/ubuntu|g' /etc/apt/sources.list; \
          apt-get -o Acquire::Retries=3 update)) \
    && apt-get install -y --no-install-recommends \
      ffmpeg \
      libsndfile1 \
      curl \
      # onnxruntime 可能需要的库
      libgomp1 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=python-builder /opt/conda /opt/conda

WORKDIR /app

COPY src/ ./src/
COPY configs/ ./configs/
COPY data/hotwords/ ./data/hotwords/

RUN mkdir -p /app/llama_cpp/lib
COPY --from=llama-builder /out/ /app/llama_cpp/lib/

RUN mkdir -p data/models data/uploads data/outputs

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
