"""核心转写引擎"""
import asyncio
import logging
import time
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from pathlib import Path

import numpy as np

from src.config import settings
from src.models.model_manager import model_manager
from src.core.hotword import PhonemeCorrector
from src.core.hotword.rule_corrector import RuleCorrector
from src.core.hotword.rectification import RectificationRAG
from src.core.speaker import SpeakerLabeler
from src.core.llm import LLMClient, LLMMessage, PromptBuilder
from src.core.llm.roles import get_role
from src.core.text_processor import TextPostProcessor, PostProcessorSettings
from src.core.text_processor.text_corrector import TextCorrector
from src.core.audio.chunker import AudioChunker

logger = logging.getLogger(__name__)


class TranscriptionEngine:
    """转写引擎 - 整合 ASR + 热词纠错 + 说话人识别 + LLM润色"""

    def __init__(self):
        self.corrector = PhonemeCorrector(
            threshold=settings.hotwords_threshold,
            similar_threshold=settings.hotwords_threshold - 0.2,
            use_faiss=settings.hotword_use_faiss,
            faiss_index_type=settings.hotword_faiss_index_type,
        )
        self.rule_corrector = RuleCorrector()
        self.rectification_rag = RectificationRAG()
        self.speaker_labeler = SpeakerLabeler()

        # 文本后处理器
        self.post_processor = TextPostProcessor.from_config(settings)

        # 长音频分块器
        self.audio_chunker = AudioChunker(
            max_chunk_duration=settings.vad_max_segment_ms / 1000.0,
            min_chunk_duration=5.0,
            overlap_duration=0.5,
        )

        # 通用文本纠错器 (pycorrector)
        self._text_corrector: Optional[TextCorrector] = None
        self._text_correct_enabled = settings.text_correct_enable

        # LLM 组件
        self._llm_client: Optional[LLMClient] = None
        self._prompt_builder: Optional[PromptBuilder] = None
        # Forced hotwords (强制替换/纠错)：用于 PhonemeCorrector + rules 等纠错链路。
        self._hotwords_list: List[str] = []
        # Context hotwords (上下文提示)：仅用于前向注入/提示模型，不做强制替换。
        self._context_hotwords_list: List[str] = []

        self._hotwords_loaded = False
        self._context_hotwords_loaded = False
        self._rules_loaded = False
        self._rectify_loaded = False

    def warmup(self, duration: float = 1.0) -> Dict[str, Any]:
        """预热模型，消除首次推理延迟

        Args:
            duration: 预热音频时长(秒)

        Returns:
            预热结果统计
        """
        import numpy as np

        results = {
            "backend": model_manager.backend.get_info()["name"],
            "warmup_duration": duration,
            "timings": {}
        }

        # 生成静默音频
        sample_rate = 16000
        samples = int(sample_rate * duration)
        silent_audio = np.zeros(samples, dtype=np.float32)

        logger.info(f"Warming up transcription engine ({duration}s audio)...")

        # 预热 ASR 后端
        backend = model_manager.backend
        start_time = time.time()

        try:
            # 尝试调用后端的 warmup 方法
            if hasattr(backend, 'warmup'):
                backend.warmup(duration)
            else:
                # 直接进行一次推理
                _ = backend.transcribe(silent_audio)

            results["timings"]["asr"] = time.time() - start_time
            logger.info(f"ASR warmup completed in {results['timings']['asr']:.2f}s")
        except Exception as e:
            logger.warning(f"ASR warmup failed: {e}")
            results["timings"]["asr"] = -1

        # 预热热词纠错器
        start_time = time.time()
        try:
            if self._hotwords_loaded:
                _ = self.corrector.correct("测试预热文本")
                results["timings"]["hotword"] = time.time() - start_time
        except Exception as e:
            logger.warning(f"Hotword warmup failed: {e}")

        logger.info("Engine warmup completed")
        return results

    @property
    def llm_client(self) -> LLMClient:
        """懒加载 LLM 客户端"""
        if self._llm_client is None:
            self._llm_client = LLMClient(
                base_url=settings.llm_base_url,
                model=settings.llm_model,
                api_key=settings.llm_api_key,
                backend=settings.llm_backend,
                max_tokens=settings.llm_max_tokens,
                cache_enable=settings.llm_cache_enable,
                cache_size=settings.llm_cache_size,
                cache_ttl=settings.llm_cache_ttl,
            )
        return self._llm_client

    @property
    def text_corrector(self) -> Optional[TextCorrector]:
        """懒加载文本纠错器"""
        if self._text_corrector is None and self._text_correct_enabled:
            try:
                self._text_corrector = TextCorrector(
                    backend=settings.text_correct_backend,
                    device=settings.text_correct_device,
                )
            except Exception as e:
                logger.error(f"Failed to initialize TextCorrector: {e}")
                self._text_correct_enabled = False
        return self._text_corrector

    def load_hotwords(self, path: Optional[str] = None):
        """加载热词"""
        if path is None:
            path = str(settings.hotwords_dir / settings.hotwords_file)

        if Path(path).exists():
            count = self.corrector.load_hotwords_file(path)
            # 缓存热词列表供 LLM 使用
            with open(path, 'r', encoding='utf-8') as f:
                self._hotwords_list = [
                    line.strip() for line in f
                    if line.strip() and not line.startswith('#')
                ]
            logger.info(f"Loaded {count} hotwords from {path}")
            self._hotwords_loaded = True
        else:
            logger.warning(f"Hotwords file not found: {path}")

    def load_context_hotwords(self, path: Optional[str] = None) -> None:
        """加载上下文热词（仅用于注入提示，不强制替换）"""
        if path is None:
            path = str(settings.hotwords_dir / settings.hotwords_context_file)

        p = Path(path)
        if p.exists():
            with p.open("r", encoding="utf-8") as f:
                self._context_hotwords_list = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.startswith("#")
                ]
            logger.info(f"Loaded {len(self._context_hotwords_list)} context hotwords from {path}")
            self._context_hotwords_loaded = True
        else:
            logger.warning(f"Context hotwords file not found: {path}")

    def load_rules(self, path: Optional[str] = None):
        """加载规则"""
        if path is None:
            path = str(settings.hotwords_dir / "hot-rules.txt")

        if Path(path).exists():
            count = self.rule_corrector.load_rules_file(path)
            logger.info(f"Loaded {count} rules from {path}")
            self._rules_loaded = True
        else:
            logger.warning(f"Rules file not found: {path}")

    def load_rectify_history(self, path: Optional[str] = None):
        """加载纠错历史"""
        if path is None:
            path = str(settings.hotwords_dir / "hot-rectify.txt")

        if Path(path).exists():
            self.rectification_rag = RectificationRAG(rectify_file=path)
            count = self.rectification_rag.load_history()
            logger.info(f"Loaded {count} rectify records from {path}")
            self._rectify_loaded = True
        else:
            logger.warning(f"Rectify file not found: {path}")

    def load_all(self):
        """加载所有热词相关文件"""
        self.load_hotwords()
        self.load_context_hotwords()
        self.load_rules()
        self.load_rectify_history()

    def update_hotwords(self, hotwords: Union[str, List[str]]):
        """更新热词"""
        if isinstance(hotwords, list):
            self._hotwords_list = hotwords
            hotwords = "\n".join(hotwords)
        else:
            self._hotwords_list = [
                line.strip() for line in hotwords.split('\n')
                if line.strip() and not line.startswith('#')
            ]

        count = self.corrector.update_hotwords(hotwords)
        logger.info(f"Updated {count} hotwords")
        self._hotwords_loaded = True

    def update_context_hotwords(self, hotwords: Union[str, List[str]]) -> None:
        """更新上下文热词（仅用于注入提示，不强制替换）"""
        if isinstance(hotwords, list):
            self._context_hotwords_list = hotwords
        else:
            self._context_hotwords_list = [
                line.strip()
                for line in str(hotwords).split("\n")
                if line.strip() and not line.startswith("#")
            ]

        logger.info(f"Updated {len(self._context_hotwords_list)} context hotwords")
        self._context_hotwords_loaded = True

    def _get_injection_hotwords(self, custom_hotwords: Optional[str] = None) -> Optional[str]:
        """获取用于前向注入的热词字符串

        Args:
            custom_hotwords: 自定义热词（优先使用）

        Returns:
            热词字符串（换行分隔）或 None
        """
        # 如果提供了自定义热词，直接使用
        if custom_hotwords:
            return custom_hotwords

        # 检查是否启用前向注入
        if not settings.hotword_injection_enable:
            return None

        # 检查是否有已加载的热词
        if not self._context_hotwords_list and not self._hotwords_list:
            return None

        # 截取最大数量并拼接
        max_count = settings.hotword_injection_max
        # Prefer context hotwords for injection (更安全)，fallback to forced list.
        hotwords_to_inject = (
            self._context_hotwords_list[:max_count]
            if self._context_hotwords_list
            else self._hotwords_list[:max_count]
        )
        return "\n".join(hotwords_to_inject)

    def _get_request_post_processor(self, asr_options: Optional[Dict[str, Any]]) -> TextPostProcessor:
        """Build a request-scoped post-processor (does not mutate global settings)."""
        postprocess_options = None
        if isinstance(asr_options, dict):
            postprocess_options = asr_options.get("postprocess")

        if not isinstance(postprocess_options, dict) or not postprocess_options:
            return self.post_processor

        # Base from current Settings, then override known keys.
        pp_settings = PostProcessorSettings(
            filler_remove_enable=settings.filler_remove_enable,
            filler_aggressive=settings.filler_aggressive,
            qj2bj_enable=settings.qj2bj_enable,
            itn_enable=settings.itn_enable,
            itn_erhua_remove=settings.itn_erhua_remove,
            spacing_cjk_ascii_enable=settings.spacing_cjk_ascii_enable,
            zh_convert_enable=settings.zh_convert_enable,
            zh_convert_locale=settings.zh_convert_locale,
            punc_convert_enable=settings.punc_convert_enable,
            punc_add_space=settings.punc_add_space,
            punc_restore_enable=settings.punc_restore_enable,
            punc_restore_model=settings.punc_restore_model,
            punc_restore_device=settings.device,
            punc_merge_enable=settings.punc_merge_enable,
            trash_punc_enable=settings.trash_punc_enable,
            trash_punc_chars=settings.trash_punc_chars,
        )
        for k, v in postprocess_options.items():
            if hasattr(pp_settings, k):
                setattr(pp_settings, k, v)

        return TextPostProcessor(pp_settings)

    def _get_request_chunker(self, asr_options: Optional[Dict[str, Any]]) -> AudioChunker:
        """Build a request-scoped AudioChunker based on `asr_options.chunking`."""
        import math

        chunking_options = None
        if isinstance(asr_options, dict):
            chunking_options = asr_options.get("chunking")

        if not isinstance(chunking_options, dict) or not chunking_options:
            return self.audio_chunker

        # Base from current engine chunker.
        silence_threshold_db = -40.0
        try:
            if getattr(self.audio_chunker, "silence_threshold", None):
                silence_threshold_db = 20.0 * math.log10(float(self.audio_chunker.silence_threshold))
        except Exception:
            silence_threshold_db = -40.0

        base_strategy = getattr(self.audio_chunker, "strategy", "silence")
        strategy = chunking_options.get("strategy", base_strategy)
        if not isinstance(strategy, str) or not strategy.strip():
            strategy = base_strategy

        max_chunk = float(chunking_options.get("max_chunk_duration_s", self.audio_chunker.max_chunk_duration))
        min_chunk = float(chunking_options.get("min_chunk_duration_s", self.audio_chunker.min_chunk_duration))
        overlap = float(chunking_options.get("overlap_duration_s", self.audio_chunker.overlap_duration))
        silence_db = float(chunking_options.get("silence_threshold_db", silence_threshold_db))
        min_silence = float(chunking_options.get("min_silence_duration_s", self.audio_chunker.min_silence_duration))

        # Best-effort sanity constraints.
        if max_chunk <= 0:
            max_chunk = self.audio_chunker.max_chunk_duration
        if min_chunk < 0:
            min_chunk = self.audio_chunker.min_chunk_duration
        if overlap < 0:
            overlap = self.audio_chunker.overlap_duration

        return AudioChunker(
            max_chunk_duration=max_chunk,
            min_chunk_duration=min_chunk,
            overlap_duration=overlap,
            silence_threshold_db=silence_db,
            min_silence_duration=min_silence,
            strategy=str(strategy).strip().lower(),
        )

    def _get_request_backend_kwargs(self, asr_options: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Return backend kwargs derived from `asr_options.backend` (reserved keys removed)."""
        backend_options = None
        if isinstance(asr_options, dict):
            backend_options = asr_options.get("backend")

        if not isinstance(backend_options, dict) or not backend_options:
            return {}

        reserved = {
            "input",
            "audio_input",
            "hotword",
            "hotwords",
            "with_speaker",
            "cache",
            "is_final",
        }
        return {k: v for k, v in backend_options.items() if isinstance(k, str) and k not in reserved}

    def _apply_corrections(
        self,
        text: str,
        *,
        post_processor: Optional[TextPostProcessor] = None,
        correction_pipeline: Optional[str] = None,
    ) -> Tuple[str, List[Tuple[str, str, float]]]:
        """应用纠错管线

        按 correction_pipeline 配置的顺序执行各纠错步骤。
        默认顺序: hotword → rules → pycorrector → post_process

        Returns:
            (纠错后文本, 相似词候选列表 [(原词, 热词, 分数), ...])
        """
        original = text
        all_similars: List[Tuple[str, str, float]] = []
        pipeline_str = correction_pipeline or settings.correction_pipeline
        pipeline = [s.strip() for s in pipeline_str.split(',') if s.strip()]
        pp = post_processor or self.post_processor

        for step in pipeline:
            if step == "hotword" and self._hotwords_loaded and text:
                prev = text
                correction = self.corrector.correct(text)
                text = correction.text
                all_similars.extend(correction.similars)
                if text != prev:
                    logger.debug(f"Hotword correction: {prev!r} -> {text!r}")

            elif step == "rules" and self._rules_loaded and text:
                prev = text
                text = self.rule_corrector.substitute(text)
                if text != prev:
                    logger.debug(f"Rule correction: {prev!r} -> {text!r}")

            elif step == "pycorrector" and self._text_correct_enabled and text and self.text_corrector:
                prev = text
                text, errors = self.text_corrector.correct(text)
                if text != prev:
                    logger.debug(f"Text correction: {prev!r} -> {text!r}, errors={errors}")

            elif step == "post_process":
                text = pp.process(text)

        if text != original:
            logger.debug(f"Total correction: {original!r} -> {text!r}")

        return text, all_similars

    def _filter_low_confidence(
        self,
        sentence_info: List[Dict[str, Any]],
        threshold: float = 0.6,
    ) -> List[Dict[str, Any]]:
        """标记并处理低置信度片段

        对低置信度的句子进行额外纠错处理。
        支持 pycorrector 和 LLM 两种回退策略。

        Args:
            sentence_info: 句子信息列表
            threshold: 置信度阈值

        Returns:
            处理后的句子信息列表
        """
        if threshold <= 0:
            return sentence_info

        fallback = settings.confidence_fallback

        for sent in sentence_info:
            confidence = sent.get('confidence', 1.0)
            if confidence < threshold:
                sent['low_confidence'] = True
                text = sent.get('text', '')

                if not text:
                    continue

                if fallback == "pycorrector" and self.text_corrector:
                    corrected, _ = self.text_corrector.correct(text)
                    if corrected != text:
                        logger.debug(f"Low confidence ({confidence:.2f}) pycorrector: {text!r} -> {corrected!r}")
                        sent['text'] = corrected
                        sent['correction_method'] = 'pycorrector'

                elif fallback == "llm" and settings.llm_enable:
                    # 使用 LLM 进行纠错 (异步转同步)
                    try:
                        import asyncio
                        try:
                            loop = asyncio.get_event_loop()
                            corrected = loop.run_until_complete(
                                self._apply_llm_polish(text, role="corrector")
                            )
                        except RuntimeError:
                            corrected = asyncio.run(
                                self._apply_llm_polish(text, role="corrector")
                            )
                        if corrected and corrected != text:
                            logger.debug(f"Low confidence ({confidence:.2f}) LLM: {text!r} -> {corrected!r}")
                            sent['text'] = corrected
                            sent['correction_method'] = 'llm'
                    except Exception as e:
                        logger.warning(f"LLM correction failed for low confidence segment: {e}")

        return sentence_info

    @staticmethod
    def _dedupe_similars(similars: List[Tuple[str, str, float]]) -> List[Tuple[str, str, float]]:
        """去重相似词候选，同一 (原词, 热词) 对只保留最高分"""
        seen: Dict[tuple, float] = {}
        for orig, hw, score in similars:
            key = (orig, hw)
            if key not in seen or score > seen[key]:
                seen[key] = score
        return [(k[0], k[1], v) for k, v in sorted(seen.items(), key=lambda x: -x[1])]

    async def _apply_llm_polish(
        self,
        text: str,
        role: str = "default",
        prev_context: Optional[str] = None,
        next_context: Optional[str] = None,
        similarity_candidates: Optional[List[Tuple[str, str, float]]] = None,
    ) -> str:
        """应用 LLM 润色

        Args:
            text: 待润色文本
            role: LLM 角色
            prev_context: 前文上下文
            next_context: 后文上下文
            similarity_candidates: 相似词候选 [(原词, 热词, 分数), ...]
        """
        if not text:
            return text

        # 获取角色
        role_obj = get_role(role)

        # 构建提示词
        prompt_builder = PromptBuilder(system_prompt=role_obj.system_prompt)

        # 获取纠错历史上下文
        rectify_context = None
        if self._rectify_loaded:
            results = self.rectification_rag.search(text, top_k=3)
            if results:
                rectify_context = self.rectification_rag.format_prompt(results)

        # 构建消息
        messages = prompt_builder.build(
            user_content=text,
            hotwords=(
                (self._context_hotwords_list[:50] if self._context_hotwords_list else None)
                or (self._hotwords_list[:50] if self._hotwords_list else None)
            ),
            similarity_candidates=similarity_candidates,
            rectify_context=rectify_context,
            prev_context=prev_context,
            next_context=next_context,
            include_history=False
        )

        # 转换为 LLMMessage
        llm_messages = [LLMMessage(role=m["role"], content=m["content"]) for m in messages]

        # 调用 LLM
        result_parts = []
        async for chunk in self.llm_client.chat(llm_messages, stream=False):
            result_parts.append(chunk)

        polished = "".join(result_parts).strip()
        return polished if polished else text

    async def _apply_llm_fulltext_polish(
        self,
        text: str,
        max_chars: int = 2000,
        similarity_candidates: Optional[List[Tuple[str, str, float]]] = None,
    ) -> str:
        """应用 LLM 全文纠错

        使用专门的 corrector 角色对全文进行一次性纠错，
        利用完整上下文提升一致性。

        Args:
            text: 待纠错全文
            max_chars: 最大字符数限制
            similarity_candidates: 相似词候选 [(原词, 热词, 分数), ...]

        Returns:
            纠错后的文本
        """
        if not text:
            return text

        # 超长文本截断
        if len(text) > max_chars:
            logger.warning(f"Text too long for fulltext polish ({len(text)} > {max_chars}), truncating")
            text = text[:max_chars]

        # 使用 corrector 角色
        role_obj = get_role("corrector")
        prompt_builder = PromptBuilder(system_prompt=role_obj.system_prompt)

        # 获取纠错历史上下文
        rectify_context = None
        if self._rectify_loaded:
            results = self.rectification_rag.search(text[:200], top_k=5)
            if results:
                rectify_context = self.rectification_rag.format_prompt(results)

        # 构建消息
        messages = prompt_builder.build(
            user_content=role_obj.format_user_input(text),
            hotwords=(
                (self._context_hotwords_list[:50] if self._context_hotwords_list else None)
                or (self._hotwords_list[:50] if self._hotwords_list else None)
            ),
            similarity_candidates=similarity_candidates,
            rectify_context=rectify_context,
            include_history=False
        )

        # 转换为 LLMMessage
        llm_messages = [LLMMessage(role=m["role"], content=m["content"]) for m in messages]

        # 调用 LLM
        result_parts = []
        async for chunk in self.llm_client.chat(llm_messages, stream=False):
            result_parts.append(chunk)

        polished = "".join(result_parts).strip()
        if polished:
            logger.debug(f"Fulltext LLM correction applied ({len(text)} -> {len(polished)} chars)")
        return polished if polished else text

    async def _apply_llm_polish_with_context(
        self,
        sentences: List[Dict[str, Any]],
        role: str = "default",
        context_sentences: int = 1,
        similarity_candidates: Optional[List[Tuple[str, str, float]]] = None,
    ) -> List[Dict[str, Any]]:
        """对句子列表应用带上下文的 LLM 润色

        Args:
            sentences: 句子列表 [{"text": "...", ...}, ...]
            role: LLM 角色
            context_sentences: 上下文句子数
            similarity_candidates: 相似词候选 [(原词, 热词, 分数), ...]

        Returns:
            润色后的句子列表
        """
        if not sentences or context_sentences <= 0:
            # 不使用上下文，逐句处理
            for sent in sentences:
                if sent.get("text"):
                    sent["text"] = await self._apply_llm_polish(
                        sent["text"], role=role, similarity_candidates=similarity_candidates
                    )
            return sentences

        # 使用上下文处理
        for i, sent in enumerate(sentences):
            if not sent.get("text"):
                continue

            # 构建上下文
            prev_texts = []
            for j in range(max(0, i - context_sentences), i):
                if sentences[j].get("text"):
                    prev_texts.append(sentences[j]["text"])

            next_texts = []
            for j in range(i + 1, min(len(sentences), i + 1 + context_sentences)):
                if sentences[j].get("text"):
                    next_texts.append(sentences[j]["text"])

            prev_context = " ".join(prev_texts) if prev_texts else None
            next_context = " ".join(next_texts) if next_texts else None

            sent["text"] = await self._apply_llm_polish(
                sent["text"],
                role=role,
                prev_context=prev_context,
                next_context=next_context,
                similarity_candidates=similarity_candidates,
            )

        return sentences

    async def _apply_llm_batch_polish(
        self,
        sentences: List[Dict[str, Any]],
        role: str = "default",
        batch_size: int = 5,
    ) -> List[Dict[str, Any]]:
        """批量 LLM 润色 - 将多个句子合并为一个请求

        将多个句子合并发送给 LLM，减少 API 调用次数，提高效率。

        Args:
            sentences: 句子列表 [{"text": "...", ...}, ...]
            role: LLM 角色
            batch_size: 每批处理的句子数

        Returns:
            润色后的句子列表
        """
        import re as re_module

        if not sentences:
            return sentences

        role_obj = get_role(role)

        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            texts = [s.get('text', '') for s in batch if s.get('text')]

            if not texts:
                continue

            # 合并为编号列表
            combined = "\n".join(f"[{j+1}] {t}" for j, t in enumerate(texts))

            prompt_builder = PromptBuilder(system_prompt=role_obj.system_prompt)
            messages = prompt_builder.build(
                user_content=f"请润色以下语音识别结果，按编号返回：\n{combined}",
                hotwords=(
                    (self._context_hotwords_list[:50] if self._context_hotwords_list else None)
                    or (self._hotwords_list[:50] if self._hotwords_list else None)
                ),
                include_history=False
            )

            llm_messages = [LLMMessage(role=m["role"], content=m["content"]) for m in messages]

            result_parts = []
            async for chunk in self.llm_client.chat(llm_messages, stream=False):
                result_parts.append(chunk)

            polished = "".join(result_parts).strip()

            # 解析返回结果
            if polished:
                pattern = re_module.compile(r'\[(\d+)\]\s*(.+?)(?=\[\d+\]|$)', re_module.DOTALL)
                matches = pattern.findall(polished)

                for num_str, text in matches:
                    idx = int(num_str) - 1
                    if 0 <= idx < len(texts):
                        # 找到对应的原始句子并更新
                        for j, s in enumerate(batch):
                            if s.get('text') == texts[idx]:
                                sentences[i + j]['text'] = text.strip()
                                break

        return sentences

    def transcribe(
        self,
        audio_input: Union[bytes, str, Path],
        with_speaker: bool = False,
        apply_hotword: bool = True,
        apply_llm: bool = False,
        llm_role: str = "default",
        hotwords: Optional[str] = None,
        asr_options: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        执行转写

        Args:
            audio_input: 音频输入（文件路径或字节）
            with_speaker: 是否进行说话人识别
            apply_hotword: 是否应用热词纠错
            apply_llm: 是否应用 LLM 润色
            llm_role: LLM 角色（default/translator/code）
            hotwords: 自定义热词（覆盖已加载的热词）
            asr_options: 每请求 ASR 调参 (preprocess/chunking/backend/postprocess)
            **kwargs: 其他参数传递给 ASR 模型

        Returns:
            转写结果字典
        """
        # 获取后端
        backend = model_manager.backend

        # 获取注入热词
        injection_hotwords = self._get_injection_hotwords(hotwords)

        # Per-request overrides (do not mutate globals).
        post_processor = self._get_request_post_processor(asr_options)
        effective_backend_kwargs: Dict[str, Any] = {}
        effective_backend_kwargs.update(self._get_request_backend_kwargs(asr_options))
        effective_backend_kwargs.update(kwargs)

        # 检查说话人识别支持
        if with_speaker and not backend.supports_speaker:
            logger.warning(
                f"Backend {backend.get_info()['name']} does not support speaker diarization, "
                "falling back to PyTorch backend"
            )
            # 回退到 loader (PyTorch) 以支持说话人识别
            raw_result = model_manager.loader.transcribe(
                audio_input,
                hotwords=injection_hotwords,
                with_speaker=with_speaker,
                **effective_backend_kwargs
            )
        else:
            # 使用配置的后端
            try:
                raw_result = backend.transcribe(
                    audio_input,
                    hotwords=injection_hotwords,
                    with_speaker=with_speaker,
                    **effective_backend_kwargs
                )
            except Exception as e:
                logger.error(f"ASR transcription failed: {e}")
                raise

        # 提取文本和句子信息
        text = raw_result.get("text", "")
        sentence_info = raw_result.get("sentence_info", [])

        # 置信度过滤
        if settings.confidence_threshold > 0:
            sentence_info = self._filter_low_confidence(
                sentence_info, threshold=settings.confidence_threshold
            )

        # 热词纠错 - 收集相似词候选
        all_similars: List[Tuple[str, str, float]] = []
        if apply_hotword:
            text, similars = self._apply_corrections(text, post_processor=post_processor)
            all_similars.extend(similars)
            # 同时纠错每个句子的文本
            for sent in sentence_info:
                sent["text"], sent_similars = self._apply_corrections(
                    sent.get("text", ""),
                    post_processor=post_processor,
                )
                all_similars.extend(sent_similars)

        # 去重相似词候选
        all_similars = self._dedupe_similars(all_similars)

        # LLM 润色 - 传入相似词候选
        if apply_llm:
            try:
                if settings.llm_fulltext_enable:
                    text = asyncio.get_event_loop().run_until_complete(
                        self._apply_llm_fulltext_polish(
                            text,
                            max_chars=settings.llm_fulltext_max_chars,
                            similarity_candidates=all_similars
                        )
                    )
                else:
                    text = asyncio.get_event_loop().run_until_complete(
                        self._apply_llm_polish(text, role=llm_role, similarity_candidates=all_similars)
                    )
            except RuntimeError:
                if settings.llm_fulltext_enable:
                    text = asyncio.run(
                        self._apply_llm_fulltext_polish(
                            text,
                            max_chars=settings.llm_fulltext_max_chars,
                            similarity_candidates=all_similars
                        )
                    )
                else:
                    text = asyncio.run(
                        self._apply_llm_polish(text, role=llm_role, similarity_candidates=all_similars)
                    )

        # 说话人标注
        if with_speaker and sentence_info:
            sentence_info = self.speaker_labeler.label_speakers(sentence_info)

        # 构建返回结果
        result = {
            "text": text,
            "text_accu": None,
            "sentences": [
                {
                    "text": s.get("text", ""),
                    "start": s.get("start", 0),
                    "end": s.get("end", 0),
                    **({"speaker": s.get("speaker"), "speaker_id": s.get("speaker_id")}
                       if with_speaker else {})
                }
                for s in sentence_info
            ],
            "raw_text": raw_result.get("text", ""),
        }

        # 生成格式化转写稿
        if with_speaker:
            result["transcript"] = self.speaker_labeler.format_transcript(
                result["sentences"],
                include_timestamp=True
            )

        return result

    async def transcribe_async(
        self,
        audio_input: Union[bytes, str, Path],
        with_speaker: bool = False,
        apply_hotword: bool = True,
        apply_llm: bool = False,
        llm_role: str = "default",
        hotwords: Optional[str] = None,
        asr_options: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        异步执行转写（适用于 FastAPI 异步端点）

        Args:
            同 transcribe()

        Returns:
            转写结果字典
        """
        # 获取后端
        backend = model_manager.backend

        # 获取注入热词
        injection_hotwords = self._get_injection_hotwords(hotwords)

        # Per-request overrides (do not mutate globals).
        post_processor = self._get_request_post_processor(asr_options)
        effective_backend_kwargs: Dict[str, Any] = {}
        effective_backend_kwargs.update(self._get_request_backend_kwargs(asr_options))
        effective_backend_kwargs.update(kwargs)

        # 检查说话人识别支持
        if with_speaker and not backend.supports_speaker:
            logger.warning(
                f"Backend {backend.get_info()['name']} does not support speaker diarization, "
                "falling back to PyTorch backend"
            )
            raw_result = model_manager.loader.transcribe(
                audio_input,
                hotwords=injection_hotwords,
                with_speaker=with_speaker,
                **effective_backend_kwargs
            )
        else:
            # 使用配置的后端
            try:
                raw_result = backend.transcribe(
                    audio_input,
                    hotwords=injection_hotwords,
                    with_speaker=with_speaker,
                    **effective_backend_kwargs
                )
            except Exception as e:
                logger.error(f"ASR transcription failed: {e}")
                raise

        # 提取文本和句子信息
        text = raw_result.get("text", "")
        sentence_info = raw_result.get("sentence_info", [])

        # 置信度过滤
        if settings.confidence_threshold > 0:
            sentence_info = self._filter_low_confidence(
                sentence_info, threshold=settings.confidence_threshold
            )

        # 热词纠错 - 收集相似词候选
        all_similars: List[Tuple[str, str, float]] = []
        if apply_hotword:
            text, similars = self._apply_corrections(text, post_processor=post_processor)
            all_similars.extend(similars)
            for sent in sentence_info:
                sent["text"], sent_similars = self._apply_corrections(
                    sent.get("text", ""),
                    post_processor=post_processor,
                )
                all_similars.extend(sent_similars)

        # 去重相似词候选
        all_similars = self._dedupe_similars(all_similars)

        # LLM 润色（异步）- 传入相似词候选
        if apply_llm:
            if settings.llm_fulltext_enable:
                text = await self._apply_llm_fulltext_polish(
                    text,
                    max_chars=settings.llm_fulltext_max_chars,
                    similarity_candidates=all_similars
                )
            else:
                text = await self._apply_llm_polish(
                    text, role=llm_role, similarity_candidates=all_similars
                )

        # 说话人标注
        if with_speaker and sentence_info:
            sentence_info = self.speaker_labeler.label_speakers(sentence_info)

        # 构建返回结果
        result = {
            "text": text,
            "text_accu": None,
            "sentences": [
                {
                    "text": s.get("text", ""),
                    "start": s.get("start", 0),
                    "end": s.get("end", 0),
                    **({"speaker": s.get("speaker"), "speaker_id": s.get("speaker_id")}
                       if with_speaker else {})
                }
                for s in sentence_info
            ],
            "raw_text": raw_result.get("text", ""),
        }

        if with_speaker:
            result["transcript"] = self.speaker_labeler.format_transcript(
                result["sentences"],
                include_timestamp=True
            )

        return result

    async def transcribe_auto_async(
        self,
        audio_input: Union[bytes, str, Path],
        with_speaker: bool = False,
        apply_hotword: bool = True,
        apply_llm: bool = False,
        llm_role: str = "default",
        hotwords: Optional[str] = None,
        asr_options: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Async auto-routing: use chunked transcription for long PCM inputs.

        This is intended for HTTP file transcription where uploads are converted to
        16kHz mono PCM16LE bytes.
        """
        # Fast path: if we can cheaply estimate duration from PCM bytes, route long audio.
        if isinstance(audio_input, (bytes, bytearray)):
            b = bytes(audio_input)
            try:
                from src.core.audio.pcm import is_wav_bytes
            except Exception:
                is_wav_bytes = lambda _d: False  # type: ignore[assignment]

            duration_s = 0.0
            if is_wav_bytes(b):
                # WAV bytes: compute duration using stdlib `wave` without decoding full audio.
                try:
                    import io
                    import wave

                    with wave.open(io.BytesIO(b), "rb") as wf:
                        frames = wf.getnframes()
                        sr = wf.getframerate() or 1
                        duration_s = float(frames) / float(sr)
                except Exception:
                    duration_s = 0.0
            else:
                # Raw PCM16LE 16k mono.
                duration_s = float(len(b)) / float(2 * 16000)

            max_chunk_duration_s = float(self._get_request_chunker(asr_options).max_chunk_duration)
            if duration_s > max_chunk_duration_s:
                # Chunked path is heavier; run it off the event loop.
                return await asyncio.to_thread(
                    self.transcribe_long_audio,
                    audio_input,
                    with_speaker=with_speaker,
                    apply_hotword=apply_hotword,
                    apply_llm=apply_llm,
                    llm_role=llm_role,
                    hotwords=hotwords,
                    asr_options=asr_options,
                    **kwargs,
                )

        return await self.transcribe_async(
            audio_input,
            with_speaker=with_speaker,
            apply_hotword=apply_hotword,
            apply_llm=apply_llm,
            llm_role=llm_role,
            hotwords=hotwords,
            asr_options=asr_options,
            **kwargs,
        )

    def transcribe_long_audio(
        self,
        audio_input: Union[bytes, str, Path, np.ndarray],
        with_speaker: bool = False,
        apply_hotword: bool = True,
        apply_llm: bool = False,
        llm_role: str = "default",
        hotwords: Optional[str] = None,
        asr_options: Optional[Dict[str, Any]] = None,
        max_workers: int = 2,
        sample_rate: int = 16000,
        **kwargs
    ) -> Dict[str, Any]:
        """
        长音频智能分块转写

        使用 VAD 检测智能分割长音频，并行处理多个分块，
        最后合并结果并去除重叠。

        Args:
            audio_input: 音频输入（文件路径、字节或 numpy 数组）
            with_speaker: 是否进行说话人识别
            apply_hotword: 是否应用热词纠错
            apply_llm: 是否应用 LLM 润色
            llm_role: LLM 角色
            hotwords: 自定义热词
            max_workers: 并行处理线程数
            sample_rate: 采样率
            **kwargs: 其他参数

        Returns:
            转写结果字典
        """
        from src.core.audio.pcm import (
            is_wav_bytes,
            pcm16le_bytes_to_float32,
            wav_bytes_to_float32,
            float32_to_pcm16le_bytes,
        )

        # Decode into float32 waveform for chunking, and normalize to PCM16LE bytes
        # for backend compatibility (remote backends require bytes, not numpy arrays).
        audio_pcm_bytes: Optional[bytes] = None

        # Per-request overrides (do not mutate globals).
        chunker = self._get_request_chunker(asr_options)
        post_processor = self._get_request_post_processor(asr_options)
        effective_backend_kwargs: Dict[str, Any] = {}
        effective_backend_kwargs.update(self._get_request_backend_kwargs(asr_options))
        effective_backend_kwargs.update(kwargs)

        chunking_options = None
        if isinstance(asr_options, dict):
            chunking_options = asr_options.get("chunking")
        if isinstance(chunking_options, dict):
            if isinstance(chunking_options.get("max_workers"), int):
                max_workers = int(chunking_options["max_workers"])
            overlap_chars = int(chunking_options.get("overlap_chars", 20) or 0)
            boundary_reconcile_enable = bool(chunking_options.get("boundary_reconcile_enable", False))
            boundary_reconcile_window_s = float(chunking_options.get("boundary_reconcile_window_s", 1.0) or 0.0)
        else:
            overlap_chars = 20
            boundary_reconcile_enable = False
            boundary_reconcile_window_s = 0.0

        if isinstance(audio_input, np.ndarray):
            audio = audio_input.astype(np.float32, copy=False)
            audio_pcm_bytes = float32_to_pcm16le_bytes(audio)

        elif isinstance(audio_input, (bytes, bytearray)):
            data = bytes(audio_input)
            if is_wav_bytes(data):
                audio, sr = wav_bytes_to_float32(data)
                if sr != sample_rate:
                    # Best-effort resample if librosa is available in the runtime.
                    try:
                        import librosa

                        audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)
                    except Exception as e:
                        raise ValueError(
                            f"Unsupported WAV sample_rate={sr}, expected {sample_rate}"
                        ) from e
                audio_pcm_bytes = float32_to_pcm16le_bytes(audio)
            else:
                audio_pcm_bytes = data
                audio = pcm16le_bytes_to_float32(audio_pcm_bytes)

        elif isinstance(audio_input, (str, Path)):
            p = Path(audio_input)
            data = p.read_bytes()
            if not is_wav_bytes(data):
                raise ValueError(
                    f"Unsupported audio file for long-audio chunking: {p.suffix}. "
                    "Please provide 16k PCM bytes (s16le) or a WAV file."
                )
            audio, sr = wav_bytes_to_float32(data)
            if sr != sample_rate:
                try:
                    import librosa

                    audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)
                except Exception as e:
                    raise ValueError(
                        f"Unsupported WAV sample_rate={sr}, expected {sample_rate}"
                    ) from e
            audio_pcm_bytes = float32_to_pcm16le_bytes(audio)

        else:
            raise ValueError(f"Unsupported audio input type: {type(audio_input)}")

        # 检查音频长度，短音频直接转写
        duration = len(audio) / sample_rate
        if duration <= chunker.max_chunk_duration:
            logger.info(f"Audio is short ({duration:.1f}s), using direct transcription")
            return self.transcribe(
                audio_pcm_bytes or audio_input,
                with_speaker=with_speaker,
                apply_hotword=apply_hotword,
                apply_llm=apply_llm,
                llm_role=llm_role,
                hotwords=hotwords,
                asr_options=asr_options,
                **kwargs
            )

        logger.info(f"Long audio detected ({duration:.1f}s), using chunked transcription")

        # Prepare backend + injection hotwords once (avoid repeating per-chunk work).
        backend = model_manager.backend
        injection_hotwords = self._get_injection_hotwords(hotwords)

        if with_speaker and not backend.supports_speaker:
            logger.warning(
                f"Backend {backend.get_info()['name']} does not support speaker diarization, "
                "falling back to PyTorch backend for long-audio chunking"
            )

        # 分割音频
        chunks = chunker.split(audio, sample_rate)

        # 定义单块转写函数
        def transcribe_chunk(chunk_audio: np.ndarray) -> Dict[str, Any]:
            # Always use PCM bytes for backend compatibility (remote backends don't accept numpy).
            chunk_bytes = float32_to_pcm16le_bytes(chunk_audio)

            if with_speaker and not backend.supports_speaker:
                raw_result = model_manager.loader.transcribe(
                    chunk_bytes,
                    hotwords=injection_hotwords,
                    with_speaker=with_speaker,
                    **effective_backend_kwargs,
                )
            else:
                raw_result = backend.transcribe(
                    chunk_bytes,
                    hotwords=injection_hotwords,
                    with_speaker=with_speaker,
                    **effective_backend_kwargs,
                )

            return {
                "text": raw_result.get("text", ""),
                "sentences": raw_result.get("sentence_info", []),
            }

        # 并行处理分块
        chunk_results = chunker.process_parallel(
            chunks, transcribe_chunk, max_workers=max_workers
        )

        # Optional boundary reconciliation (accuracy-first, slower):
        # Re-transcribe a small window around each chunk split and inject it as a
        # "bridge" result to reduce boundary misses/duplication.
        #
        # NOTE: We currently skip this when diarization is enabled because it may
        # desync `text` vs `sentences`/`transcript` (bridge results don't have stable speaker segments).
        if boundary_reconcile_enable and boundary_reconcile_window_s > 0.0 and not with_speaker:
            try:
                from src.core.audio.boundary_reconcile import build_boundary_bridge_results

                def _transcribe_bridge(pcm16le: bytes) -> str:
                    raw_bridge = backend.transcribe(
                        pcm16le,
                        hotwords=injection_hotwords,
                        with_speaker=False,
                        **effective_backend_kwargs,
                    )
                    return str(raw_bridge.get("text", "") or "")

                bridge_results = build_boundary_bridge_results(
                    audio,
                    chunk_results,
                    sample_rate=sample_rate,
                    overlap_duration_s=chunker.overlap_duration,
                    window_half_s=boundary_reconcile_window_s,
                    transcribe_pcm16le=_transcribe_bridge,
                )
                if bridge_results:
                    logger.info(
                        f"Boundary reconcile enabled: injecting {len(bridge_results)} bridge windows "
                        f"(window_half={boundary_reconcile_window_s:.2f}s)"
                    )
                    chunk_results = chunk_results + bridge_results
            except Exception as e:
                logger.warning(f"Boundary reconcile failed (ignored): {e}")

        # 合并结果
        merged = chunker.merge_results(chunk_results, sample_rate, overlap_chars=overlap_chars)

        raw_text = merged.get("text", "")
        raw_text_accu = merged.get("text_accu", "") or ""
        sentence_info = merged.get("sentences", [])

        text = raw_text
        text_accu = raw_text_accu

        all_similars: List[Tuple[str, str, float]] = []

        # 合并后统一应用纠错与后处理，避免破坏 chunk overlap 对齐。
        if apply_hotword:
            if text:
                text, similars = self._apply_corrections(text, post_processor=post_processor)
                all_similars.extend(similars)

            if text_accu:
                text_accu, similars_accu = self._apply_corrections(
                    text_accu, post_processor=post_processor
                )
                all_similars.extend(similars_accu)

            # 同时纠错每个句子的文本
            for sent in sentence_info:
                sent["text"], sent_similars = self._apply_corrections(
                    sent.get("text", ""),
                    post_processor=post_processor,
                )
                all_similars.extend(sent_similars)

        # 去重相似词候选
        all_similars = self._dedupe_similars(all_similars)

        # 全文 LLM 润色 - 传入相似词候选
        if apply_llm and text:
            try:
                if settings.llm_fulltext_enable:
                    text = asyncio.get_event_loop().run_until_complete(
                        self._apply_llm_fulltext_polish(
                            text,
                            max_chars=settings.llm_fulltext_max_chars,
                            similarity_candidates=all_similars
                        )
                    )
                else:
                    text = asyncio.get_event_loop().run_until_complete(
                        self._apply_llm_polish(text, role=llm_role, similarity_candidates=all_similars)
                    )
            except RuntimeError:
                if settings.llm_fulltext_enable:
                    text = asyncio.run(
                        self._apply_llm_fulltext_polish(
                            text,
                            max_chars=settings.llm_fulltext_max_chars,
                            similarity_candidates=all_similars
                        )
                    )
                else:
                    text = asyncio.run(
                        self._apply_llm_polish(text, role=llm_role, similarity_candidates=all_similars)
                    )

        # 说话人标注
        if with_speaker and sentence_info:
            sentence_info = self.speaker_labeler.label_speakers(sentence_info)

        result = {
            "text": text,
            "text_accu": text_accu if text_accu else None,
            "sentences": [
                {
                    "text": s.get("text", ""),
                    "start": s.get("start", 0),
                    "end": s.get("end", 0),
                    **({"speaker": s.get("speaker"), "speaker_id": s.get("speaker_id")}
                       if with_speaker else {})
                }
                for s in sentence_info
            ],
            "raw_text": raw_text,
            "duration": duration,
            "chunks": len(chunks),
        }

        if with_speaker:
            result["transcript"] = self.speaker_labeler.format_transcript(
                result["sentences"],
                include_timestamp=True
            )

        logger.info(f"Long audio transcription completed: {len(chunks)} chunks, {duration:.1f}s")
        return result

    def transcribe_streaming(
        self,
        audio_chunk: bytes,
        cache: Dict[str, Any],
        is_final: bool = False,
        hotwords: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """流式转写 (单个音频块)

        注意: 流式转写仅支持 PyTorch 后端。
        其他后端会自动回退到 PyTorch。
        """
        backend = model_manager.backend

        # 获取注入热词
        injection_hotwords = self._get_injection_hotwords(hotwords)

        # 检查流式支持
        if not backend.supports_streaming:
            logger.debug(
                f"Backend {backend.get_info()['name']} does not support streaming, "
                "using PyTorch backend for streaming"
            )
            # 使用 PyTorch 后端的流式功能
            return model_manager.loader._backend.transcribe_streaming(
                audio_chunk,
                cache,
                is_final=is_final,
                hotwords=injection_hotwords,
                **kwargs
            )

        # 使用后端的流式转写
        result = backend.transcribe_streaming(
            audio_chunk,
            cache,
            is_final=is_final,
            hotwords=injection_hotwords,
            **kwargs
        )

        # 应用纠错
        if result.get("text"):
            result["text"], _ = self._apply_corrections(result["text"])

        return result


# 全局引擎实例
transcription_engine = TranscriptionEngine()
